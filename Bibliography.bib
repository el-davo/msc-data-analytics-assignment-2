@Article{nonlinearRelationships,
 title={Nonlinear relationships},
 url={https://www3.nd.edu/~rwilliam/stats2/l61.pdf},
 note={(Last revised February 20, 2015)},
}

@Article{residualsVsFitsPlot,
 title={Residuals vs. Fits Plot},
 url={https://onlinecourses.science.psu.edu/stat501/node/277}
}

@inbook{doi:10.1002/9781118625590.ch2,
author = {Norman R. Draper and Harry Smith},
publisher = {Wiley-Blackwell},
isbn = {9781118625590},
title = {Checking the Straight Line Fit},
booktitle = {Applied Regression Analysis},
chapter = {2},
pages = {47-77},
doi = {10.1002/9781118625590.ch2},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118625590.ch2},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118625590.ch2},
year = {2014},
keywords = {data, Durbin‐Watson test, F‐test, fitted regression model, linear model, straight line fit},
abstract = {Summary This chapter discusses the basic methods of checking a fitted regression model. It talks about these in terms of fitting a straight line, and the basic methods apply generally whenever a linear model is fitted, no matter how many predictors there are. The chapter examines the lack of fit F‐test when the data contain repeat observations. It also analyzes the basic visual checks that can be made on the residuals and the Durbin‐Watson test for checking serial correlation. In the simple case of fitting a straight line, bias error can usually be detected merely by examining a plot of the data. The chapter summarizes the steps to be taken when the data contain repeat observations.}
}

@article{doi:10.1080/00031305.1975.10479105,
author = { Donald W.Marquardt  and  Ronald D.Snee },
title = {Ridge Regression in Practice},
journal = {The American Statistician},
volume = {29},
number = {1},
pages = {3-20},
year  = {1975},
publisher = {Taylor & Francis},
doi = {10.1080/00031305.1975.10479105},
URL = {https://www.tandfonline.com/doi/abs/10.1080/00031305.1975.10479105},
eprint = {https://www.tandfonline.com/doi/pdf/10.1080/00031305.1975.10479105}
}

@Article{kNearestNeighboursExample,
 title={KNN (k-nearest neighbors) classification example},
 url={http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/tutorial/plot_knn_iris.html}
}

@Article{decisionTreeExample,
 title={KNN (k-nearest neighbors) classification example},
 url={https://chongyaorobin.wordpress.com/2015/07/09/study-apache-spark-mllib-on-ipython-regression-classification-decision-tree/}
}

@Article{lassoAdvantage1,
 title={Generalized Linear Models},
 url={http://scikit-learn.org/stable/modules/linear_model.html}
}

@Article{heteroscedasticity,
 title={Confusing Stats Terms Explained: Heteroscedasticity (Heteroskedasticity)
},
 url={http://www.statsmakemecry.com/smmctheblog/confusing-stats-terms-explained-heteroscedasticity-heteroske.html}
}

@INPROCEEDINGS{6024507, 
author={J. B. Kulkarni and A. A. Sawant and V. S. Inamdar}, 
booktitle={2011 International Conference on Signal Processing, Communication, Computing and Networking Technologies}, 
title={Database processing by Linear Regression on GPU using CUDA}, 
year={2011}, 
volume={}, 
number={}, 
pages={20-23}, 
keywords={computer graphic equipment;coprocessors;covariance matrices;eigenvalues and eigenfunctions;multi-threading;regression analysis;visual databases;CPU;CUDA;Compute Unified Device Architecture;GPGPU;SIMD;central processing unit;covariance matrix;data parallelism;database processing;eigenvalues;eigenvectors;general purpose graphics processing unit;image database;linear regression;multithreading technique;parallel programming;single instruction multiple data;Central Processing Unit;Computer architecture;Covariance matrix;Databases;Graphics processing unit;Linear regression;Programming;CUDA;Central Processing Unit;DirectX;Graphics Processing Unit;LTI;Residual error;Sum of square;etc}, 
doi={10.1109/ICSCCN.2011.6024507}, 
ISSN={}, 
month={July},}