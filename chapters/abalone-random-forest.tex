\section{Random Forest Regression}

A random forest is a collection or ensemble of decision trees. Results are computed using the results of many decision trees. Each decision tree is trained with a different, random subset of data. The advantage of this over using a single decision tree is that a single decision tree can lead to overfitting. One disadvantage of this is that because we have to create multiple decision trees, we consume a lot more computing power.

To measure the accuracy, the \textit{mean average percentage error} was used. MAPE is the sum of the absolute value of the difference between actual and predicted times 100\%

\begin{equation}\label{eq_mape}
MAPE = \frac{100\%}{n}\sum\limits_{i=1}^n\left|\frac{y_i-\hat{y_i}}{y_i} \right|
\end{equation}

The results of running a random forest regression against the abalone dataset were immediately impressive. The average error was 0.59 degrees with an accuracy of 94.53\%. Using the same techniques we used for our decision tree with RandomizedSearchCV, the average error was reduced to 0.347 and the accuracy increased to 96.22\%. Finally, with GridSearchCV the average error had a slight improvement from 0.347 to 0.346 and the accuracy of the model again increased from 96.22\% to 96.23\%. These are marginal improvements. For grid search and randomized search, the following hyperparameters were tuned

\begin{itemize}
  \item bootstrap
  \item max\_depth
  \item max\_features
  \item max\_features
  \item min\_samples\_leaf
  \item min\_samples\_split
  \item n\_estimators
\end{itemize}

Random Forest regression proved to be the best method for predicting the rings of the abalone.

