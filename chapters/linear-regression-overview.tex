\section{Linear Regression}

Linear Regression is a common technique in the world of predictive analytics. One common example of linear regression is the fitting of straight lines to a graph. 

[INSERT IMAGE]

Is a supervised machine learning technique [EXPAND]

The data is usually not perfect, there is often a source of noise, this could be network latency

\begin{equation}
    f(x) = mx+c
\end{equation}

This describes a straight line relationship, where y is the y axis variable and x is the x axis variable, the relationship between them is defined as m where m is the slope of the line. c is a constant or intercept where the line meets the y axis

The above image is an example of building a model with one variable, which predicts y based on the values of x using an intercept and slope values. More generally we will be fitting muliple co-efficients. Co-efficients means lots of different features. For example if we want to predict weather, then to make an accurate model we would probably need multiple features such as wind speed and atmospheric pressure.

\begin{equation}
\bar{y} = \frac{1}{n}\sum_{i=1}^n y_i
\end{equation}

Y hat is the predicted value of y. Beta 0 corresponds to to the intercept value. Sum of, is iterating over the features denoted by k. k can be described as the k co-efficient values for a feature.[EXPAND]

\subsection*{Linear Regression Features}

Linear regression works by finding a straight line relationship between a set of features (coefficients). Features could be anything from miles per gallon of a new car to the frequency of a cpu. One caveat with these features is that in linear regression they must be expressed numerically. For example if we have a feature x which denotes the gender of a human as either 'M', 'F'. We could can not input these values into our regression as they are not numeric. These values could not be used in our regression without being transformed. One of the methods of data transformation is called one hot encoding. 

\subsection*{Feature Transformations}

There are certain cases where a feature transformation is required. As mentioned earlier we would need one hot encoding to make sure our data is numeric. Using our example of a gender feature above, one hot encoding will take the string values 'M' and 'F' and convert them to numbers. Interestingly it will not replace our gender column, instead it will create 2 more features. We will end up with a new male and female feature encoded with a 1 for a true value and a 0 for a false value.

[NEEDS IMAGE OF ONE HOT ENCODING]

Transformations can be used to account for a non-linear relationship between our features and the predictor. Non linear relationships can be difficult to find, and usually relies on a deep understanding of the the data. Once we have this understanding we can start to ask questions such as, can the slope of
the relationship between Xi and E(Y) be expected to have the same sign for all values of Xi. Should we expect the magnitude of the slope to increase as Xi increases, or should we expect
the magnitude of the slope to decrease as Xi increases?\cite{7508191}

\subsection*{Advantages of linear regression}

Linear regression is a relatively simple to understand when compared to other prediction techniques such as deep neural networks.

Predictor importance can be measured by its coefficients. For example if we have 10 coefficients it is possible to measure the importance of each feature towards to the final prediction. To measure this we can look at how large the coefficient is compared to the target variable

We can apply certain transformations on our data to come up with 


\subsection*{Least Squares}
\subsection*{K Nearest Neighbours}
\subsection*{Least Squares}
\subsection*{Ridge Regression}
\subsection*{Lasso Technique}

\subsection*{Shrinkage Methods}